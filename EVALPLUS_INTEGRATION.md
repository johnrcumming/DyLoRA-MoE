# EvalPlus Integration for DyLoRA-MoE

This document explains the integration of the official EvalPlus framework into DyLoRA-MoE benchmarking.

## What is EvalPlus?

EvalPlus is the official evaluation framework for code generation models, providing:
- **HumanEval+**: 80x more tests than the original HumanEval
- **MBPP+**: 35x more tests than the original MBPP
- **Rigorous evaluation**: Standardized prompts, test execution, and scoring
- **Used by**: Meta Llama 3.1/3.3, Qwen2.5-Coder, DeepSeek-Coder V2, and many others

## Why Use EvalPlus?

### Previous Implementation Issues
Our custom benchmark implementation had several problems:
1. **Inflated scores**: Custom test execution led to 99.39% baseline pass@1 (vs 20.7% published)
2. **Structure check mismatches**: Checked raw completions instead of sanitized function code
3. **Inconsistent evaluation**: Different prompting and test execution than published benchmarks

### EvalPlus Benefits
1. **Accurate scores**: Uses the same evaluation pipeline as published benchmarks
2. **Standardized prompts**: Consistent with other model evaluations
3. **Official test suites**: Access to full HumanEval+ and MBPP+ test cases
4. **Better comparison**: Results directly comparable to published leaderboards

## Usage

### Basic Usage (Default)

**EvalPlus is now the default!** Just run:

```bash
# HumanEval with EvalPlus (default behavior)
python benchmark.py --benchmarks humaneval

# HumanEval and MBPP with EvalPlus
python benchmark.py --benchmarks humaneval mbpp

# Quick test with first 10 samples
python benchmark.py --benchmarks humaneval --max_samples 10 --no_wandb
```

No need to specify `--use_evalplus` or `--evalplus_backend hf` - they're now the defaults!

### With Trained Models

```bash
# Test trained model using EvalPlus
# Test trained model with EvalPlus (default)
python benchmark.py --trained_model ./results_full/best_model --benchmarks humaneval
```

### Using Legacy Benchmarks (Not Recommended)

If you need the old custom benchmarks, use `--no_evalplus`:

```bash
# Old custom benchmark implementation
python benchmark.py --benchmarks humaneval --no_evalplus --use_async_tests
```

## Implementation Details

### Architecture

The integration consists of three components:

1. **EvalPlusBenchmark** (`benchmarks/evalplus_benchmark.py`):
   - Wraps EvalPlus's `run_codegen()` and `evaluate()` functions
   - Handles code generation using HuggingFace transformers backend
   - Manages result parsing and metric extraction

2. **Updated run_benchmarks()** (`benchmark.py`):
   - Routes to EvalPlusBenchmark when `--use_evalplus` is set
   - Falls back to legacy benchmarks otherwise
   - Passes model name to EvalPlus for proper loading

3. **CLI Integration** (`benchmark.py`):
   - `--use_evalplus`: Enable EvalPlus framework
   - `--evalplus_backend`: Choose backend (hf, vllm, etc.)

### Workflow

When `--use_evalplus` is enabled:

1. **Generation Phase**:
   - EvalPlus loads the model via HuggingFace transformers
   - Generates code for each problem using greedy decoding
   - Saves samples to `evalplus_results/{dataset}/{model}_*.jsonl`

2. **Evaluation Phase**:
   - EvalPlus sanitizes generated code (extracts function definitions)
   - Executes test cases in a safe subprocess environment
   - Computes pass@1 for base and plus test suites
   - Saves results to `evalplus_results/{dataset}/*.eval_results.json`

3. **Metrics**:
   - `base_pass@1`: Pass rate on original HumanEval/MBPP tests
   - `plus_pass@1`: Pass rate on enhanced EvalPlus tests (more rigorous)
   - `pass@1`: Overall pass rate (typically reports plus_pass@1)

## Expected Results

With EvalPlus, you should see realistic scores:

**CodeGemma-2B Baseline (expected)**:
- HumanEval base pass@1: ~25-30%
- HumanEval+ pass@1: ~20-25%
- MBPP base pass@1: ~30-35%
- MBPP+ pass@1: ~25-30%

These align with published benchmarks and are much more realistic than our previous 99.39% inflated scores.

## Backends

EvalPlus supports multiple backends:

- **hf** (HuggingFace transformers): Default, best for single GPU
- **vllm**: Faster, better for multi-GPU or high throughput
- **openai**: For OpenAI API models
- **anthropic**: For Claude models
- **google**: For Gemini models

Currently implemented: `hf` backend only. Others can be added as needed.

## File Structure

```
benchmarks/
├── evalplus_benchmark.py      # New: EvalPlus wrapper
├── base_benchmark.py           # Base class (used by both)
├── humaneval_benchmark.py      # Legacy custom implementation
├── humanevalplus_benchmark.py  # Legacy custom implementation
└── mbpp_benchmark.py           # Legacy custom implementation

evalplus_results/               # Generated by EvalPlus
├── humaneval/
│   ├── google--codegemma-2b_hf_temp_0.0.jsonl
│   └── google--codegemma-2b_hf_temp_0.0.eval_results.json
└── mbpp/
    └── ...
```

## Troubleshooting

### ImportError: No module named 'evalplus'

```bash
pip install --upgrade evalplus
```

### Model loads slowly on CPU

EvalPlus uses HuggingFace transformers, which auto-detects device. If you have GPU available, it should use it automatically. Check with:

```bash
python -c "import torch; print(torch.cuda.is_available())"
```

### Results don't match published benchmarks exactly

Small variations (±2-3%) are normal due to:
- Different hardware/drivers
- Slightly different tokenization
- Random seed (if using sampling instead of greedy)

### Where are the generated samples?

Check `evalplus_results/{dataset}/` directory. Each run creates:
- `{model}_*.jsonl`: Generated code samples
- `{model}_*.eval_results.json`: Evaluation results

## Comparison with Legacy Implementation

| Feature | Legacy Benchmarks | EvalPlus |
|---------|------------------|----------|
| Test coverage | Limited | 80x more (HumanEval+), 35x more (MBPP+) |
| Evaluation accuracy | Prone to inflation | Industry standard |
| Prompting | Custom | Standardized |
| Comparison | Hard to compare | Directly comparable to leaderboards |
| Maintenance | Manual | Actively maintained by community |
| Speed | Fast (async) | Moderate (subprocess-based) |

## Migration Guide

If you were using the legacy benchmarks:

**Before**:
```bash
python benchmark.py --benchmarks humaneval --no_evalplus --use_async_tests
```

**After** (now the default):
```bash
python benchmark.py --benchmarks humaneval
```

**Key differences**:
- EvalPlus is now the default - no flags needed!
- Remove `--use_async_tests` (not needed with EvalPlus)
- To use legacy benchmarks, add `--no_evalplus`
- Expect lower (more realistic) pass@1 scores with EvalPlus
- Results stored in `evalplus_results/` instead of memory

## References

- EvalPlus GitHub: https://github.com/evalplus/evalplus
- EvalPlus Leaderboard: https://evalplus.github.io/leaderboard.html
- Paper (NeurIPS'23): https://openreview.net/forum?id=1qvx610Cu7
