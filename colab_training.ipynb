{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4067c3e",
   "metadata": {},
   "source": [
    "# DyLoRA-MoE Colab Training Notebook\n",
    "\n",
    "End-to-end interactive notebook to fine-tune and extend a Dynamic LoRA Mixture-of-Experts (DyLoRA-MoE) model (Gemma 3 270M) with continual skill ingestion and novelty-triggered expert growth.\n",
    "\n",
    "Key Features:\n",
    "- Hugging Face Transformers + PEFT LoRA experts\n",
    "- Dynamic expert creation upon novel skill detection\n",
    "- CodeAlpaca (Python subset) + MBPP evaluation\n",
    "- Sequence packing for token efficiency\n",
    "- W&B logging (losses, routing metrics, expert usage)\n",
    "- Optional Google Drive persistence\n",
    "\n",
    "Before running: (optional) set environment variables:\n",
    "- HF_TOKEN (if model gated)\n",
    "- WANDB_API_KEY (for Weights & Biases logging)\n",
    "\n",
    "Run cells in order. Adjust config in the Configuration cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2140879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Environment & Hardware Check\n",
    "import torch, platform, os, subprocess, sys\n",
    "print(f\"Python: {platform.python_version()}\")\n",
    "print(f\"Torch: {torch.__version__}\")\n",
    "try:\n",
    "    import transformers, peft, datasets\n",
    "    print('Transformers:', transformers.__version__)\n",
    "    print('PEFT:', peft.__version__)\n",
    "    print('Datasets:', datasets.__version__)\n",
    "except Exception as e:\n",
    "    print('Library import issue:', e)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device count:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}:\", torch.cuda.get_device_name(i))\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    print(\"Total VRAM (GB):\", round(gpu_props.total_memory/1024**3,2))\n",
    "else:\n",
    "    print(\"If you need a GPU: Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c40063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Install / Upgrade Dependencies (restart kernel if upgrading core libs)\n",
    "!pip -q install --upgrade pip\n",
    "!pip -q install transformers peft datasets accelerate bitsandbytes sentencepiece wandb einops\n",
    "!pip -q install trl\n",
    "import torch, os\n",
    "print('Torch:', torch.__version__)\n",
    "## 3. (Optional) Mount Google Drive for persistence\n",
    "USE_DRIVE = False  # set True to persist outputs across sessions\n",
    "if USE_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    BASE_DIR = '/content/drive/MyDrive/dylora_moe_run'\n",
    "else:\n",
    "    BASE_DIR = '/content/dylora_moe_run'\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "print('Base directory:', BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d52ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Configuration & Global Setup\n",
    "import random, json, time, math, numpy as np, torch\n",
    "cfg = {\n",
    "    'model_name': 'google/gemma-3-270m',\n",
    "    'max_seq_len': 512,\n",
    "    'pack_sequences': True,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 0.0,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'num_train_epochs': 3,\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    'train_batch_size': 1,\n",
    "    'eval_batch_size': 1,\n",
    "    'lora_r': 8,\n",
    "    'lora_alpha': 16,\n",
    "    'lora_dropout': 0.1,\n",
    "    'seed': 42,\n",
    "    'output_dir': BASE_DIR + '/outputs',\n",
    "    'logging_steps': 10,\n",
    "    'eval_steps': 50,\n",
    "    'save_steps': 50,\n",
    "    'early_stopping_patience': 5,\n",
    "}\n",
    "\n",
    "def set_seed(seed:int):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "set_seed(cfg['seed'])\n",
    "os.makedirs(cfg['output_dir'], exist_ok=True)\n",
    "print(json.dumps(cfg, indent=2))\n",
    "\n",
    "# 5. Imports\n",
    "import os\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import (AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling, EarlyStoppingCallback)\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "print('Imports loaded.')\n",
    "\n",
    "# 6. Download Datasets (CodeAlpaca subset + MBPP) and filtering\n",
    "print('Loading datasets...')\n",
    "code_alpaca = load_dataset('sahil2801/CodeAlpaca-20k', split='train')\n",
    "code_alpaca = code_alpaca.filter(lambda e: 'python' in e['instruction'].lower())\n",
    "code_alpaca = code_alpaca.train_test_split(test_size=0.1, seed=cfg['seed'])\n",
    "mbpp_full = load_dataset('mbpp', split='test')\n",
    "mbpp_full = mbpp_full.train_test_split(test_size=0.2, seed=cfg['seed'])\n",
    "mbpp_val_test = mbpp_full['test'].train_test_split(test_size=0.5, seed=cfg['seed'])\n",
    "mbpp_dataset = {\n",
    "    'train': mbpp_full['train'],\n",
    "    'validation': mbpp_val_test['train'],\n",
    "    'test': mbpp_val_test['test']\n",
    "}\n",
    "print('Sizes -> CodeAlpaca train:', len(code_alpaca['train']), 'val:', len(code_alpaca['test']))\n",
    "print('MBPP train:', len(mbpp_dataset['train']), 'val:', len(mbpp_dataset['validation']), 'test:', len(mbpp_dataset['test']))\n",
    "\n",
    "# 7. Tokenizer\n",
    "hf_token = os.environ.get('HF_TOKEN')\n",
    "from dylo_moe.model import DyLoRA_MoE\n",
    "from dylo_moe.utils import print_trainable_parameters\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg['model_name'], token=hf_token)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 8. Utility: sequence packing\n",
    "def pack_sequences(tokenizer, texts, max_length=512):\n",
    "    input_ids_batches, attn_batches = [], []\n",
    "    cur = []\n",
    "    for t in texts:\n",
    "        ids = tokenizer(t, add_special_tokens=False)['input_ids']\n",
    "        if len(ids) > max_length: ids = ids[:max_length]\n",
    "        if len(cur) + len(ids) > max_length:\n",
    "            if cur:\n",
    "                pad_len = max_length - len(cur)\n",
    "                input_ids_batches.append(cur + [tokenizer.pad_token_id]*pad_len)\n",
    "                attn_batches.append([1]*len(cur) + [0]*pad_len)\n",
    "            cur = []\n",
    "        cur.extend(ids)\n",
    "    if cur:\n",
    "        pad_len = max_length - len(cur)\n",
    "        input_ids_batches.append(cur + [tokenizer.pad_token_id]*pad_len)\n",
    "        attn_batches.append([1]*len(cur) + [0]*pad_len)\n",
    "    return input_ids_batches, attn_batches\n",
    "\n",
    "# 9. Preprocessing helpers\n",
    "\n",
    "def preprocess_eval(dataset, tokenizer):\n",
    "    def tok_fn(examples):\n",
    "        if 'text' in examples:\n",
    "            processed = ['\\n'.join(x) for x in examples['text']]\n",
    "        else:\n",
    "            processed = [f\"{ins}\\n{out}\" for ins, out in zip(examples['instruction'], examples['output'])]\n",
    "        return tokenizer(processed, padding='max_length', truncation=True, max_length=cfg['max_seq_len'])\n",
    "    return dataset.map(tok_fn, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811d6799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Build DyLoRA-MoE Model\n",
    "model = DyLoRA_MoE(\n",
    "    cfg['model_name'],\n",
    "    num_experts=1,\n",
    "    lora_r=cfg['lora_r'],\n",
    "    lora_alpha=cfg['lora_alpha'],\n",
    "    lora_dropout=cfg['lora_dropout'],\n",
    "    token=hf_token,\n",
    ")\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# 11. Prepare Evaluation Sets\n",
    "python_eval = preprocess_eval(code_alpaca['test'], tokenizer)\n",
    "mbpp_eval = preprocess_eval(mbpp_dataset['validation'], tokenizer)\n",
    "combined_eval = concatenate_datasets([\n",
    "    python_eval.add_column('eval_domain', [0]*len(python_eval)),\n",
    "    mbpp_eval.add_column('eval_domain', [1]*len(mbpp_eval))\n",
    "])\n",
    "\n",
    "# 12. Training Arguments\n",
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=cfg['output_dir'],\n",
    "    num_train_epochs=cfg['num_train_epochs'],\n",
    "    per_device_train_batch_size=cfg['train_batch_size'],\n",
    "    per_device_eval_batch_size=cfg['eval_batch_size'],\n",
    "    gradient_accumulation_steps=cfg['gradient_accumulation_steps'],\n",
    "    gradient_checkpointing=True,\n",
    "    learning_rate=cfg['learning_rate'],\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=cfg['warmup_ratio'],\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_dir=cfg['output_dir'] + '/logs',\n",
    "    logging_steps=cfg['logging_steps'],\n",
    "    logging_strategy='steps',\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=cfg['eval_steps'],\n",
    "    save_strategy='steps',\n",
    "    save_steps=cfg['save_steps'],\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    report_to=['wandb'],\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# 13. Data Collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# 14. Trainer Stub (we'll override train_dataset dynamically)\n",
    "from transformers import Trainer, EarlyStoppingCallback\n",
    "\n",
    "def compute_metrics(_):\n",
    "    return {}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collator,\n",
    "    eval_dataset=combined_eval,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=cfg['early_stopping_patience'])]\n",
    ")\n",
    "\n",
    "# 15. W&B Init\n",
    "import wandb\n",
    "if os.environ.get('WANDB_API_KEY'):\n",
    "    wandb.init(project='dylora-moe-colab')\n",
    "else:\n",
    "    os.environ['WANDB_MODE'] = 'offline'\n",
    "    wandb.init(project='dylora-moe-colab', mode='offline')\n",
    "\n",
    "# 16. Skill Streams (initial + synthetic domains)\n",
    "python_skill = [ex['output'] for ex in code_alpaca['train'].select(range(min(800, len(code_alpaca['train']))))]\n",
    "requests_skill = [\n",
    "    \"import requests\\nresp = requests.get('https://httpbin.org/get')\\nprint(resp.status_code)\",\n",
    "    \"import requests\\nresp = requests.post('https://httpbin.org/post', data={'a':1})\\nprint(resp.json())\"\n",
    "]\n",
    "flask_skill = [\n",
    "    \"from flask import Flask\\napp=Flask(__name__)\\n@app.route('/')\\ndef home():\\n    return 'hi'\",\n",
    "    \"from flask import Flask, request\\napp=Flask(__name__)\\n@app.route('/echo', methods=['POST'])\\ndef echo():\\n    return request.data.decode()\"\n",
    "]\n",
    "skills = [python_skill, requests_skill, flask_skill]\n",
    "\n",
    "# 17. Preprocessing function for training skills\n",
    "def preprocess_train(texts):\n",
    "    if cfg['pack_sequences']:\n",
    "        input_ids, attn = pack_sequences(tokenizer, texts, max_length=cfg['max_seq_len'])\n",
    "        return Dataset.from_dict({'input_ids': input_ids, 'attention_mask': attn, 'labels': input_ids})\n",
    "    tok = tokenizer(texts, padding=True, truncation=True, max_length=cfg['max_seq_len'])\n",
    "    return Dataset.from_dict({'input_ids': tok['input_ids'], 'attention_mask': tok['attention_mask'], 'labels': tok['input_ids']})\n",
    "\n",
    "# 18. Initial Evaluation\n",
    "print('Initial MBPP validation eval:')\n",
    "initial_mbpp = trainer.evaluate(mbpp_eval, metric_key_prefix='eval_mbpp')\n",
    "print(initial_mbpp)\n",
    "wandb.log({'initial_mbpp_loss': initial_mbpp['eval_mbpp_loss']})\n",
    "\n",
    "# 19. Continual Learning Loop\n",
    "last_log_time = time.time(); tokens_processed = 0\n",
    "for idx, skill in enumerate(skills):\n",
    "    print(f\"\\n=== Skill {idx+1}/{len(skills)} ===\")\n",
    "    dataset = preprocess_train(skill)\n",
    "    # Padding fraction\n",
    "    pad_tokens = sum(row.count(tokenizer.pad_token_id) for row in dataset['input_ids'])\n",
    "    total_tokens = len(dataset['input_ids'])*cfg['max_seq_len']\n",
    "    wandb.log({'padding_fraction': pad_tokens/total_tokens if total_tokens else 0})\n",
    "\n",
    "    # Novelty detection: feed batches to add_new_skill\n",
    "    device = trainer.args.device\n",
    "    is_novel = False\n",
    "    for row in dataset['input_ids']:\n",
    "        batch = torch.tensor(row).unsqueeze(0).to(device)\n",
    "        if model.add_new_skill(batch):\n",
    "            is_novel = True\n",
    "    if is_novel:\n",
    "        print('Novel skill detected -> training expert')\n",
    "        trainer.train_dataset = dataset\n",
    "        trainer.train()\n",
    "        model.router.set_expert_maturity(model.expert_manager.num_experts - 1, 1)\n",
    "        wandb.log({'num_experts': model.expert_manager.num_experts})\n",
    "    else:\n",
    "        print('Skill not novel; skipping training step')\n",
    "\n",
    "    # Routing metrics (if >1 expert)\n",
    "    if model.router.num_experts > 1:\n",
    "        sample = torch.tensor(dataset[0]['input_ids']).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.foundation_model(sample, attention_mask=(sample != tokenizer.pad_token_id), output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states[-1]\n",
    "            routing_weights = model.router(hidden_states)\n",
    "            entropy = -(routing_weights * (routing_weights.clamp(min=1e-8).log())).sum(-1).mean().item()\n",
    "            expert_usage = routing_weights.mean(dim=(0,1)).detach().cpu().tolist()\n",
    "        wandb.log({'routing_entropy': entropy, **{f'expert_usage_{i}':v for i,v in enumerate(expert_usage)}})\n",
    "\n",
    "    # Tokens/sec approximate\n",
    "    for mask in dataset['attention_mask']:\n",
    "        tokens_processed += sum(mask)\n",
    "    now = time.time()\n",
    "    if now - last_log_time >= 30:\n",
    "        tps = tokens_processed/(now - last_log_time)\n",
    "        wandb.log({'tokens_per_second': tps})\n",
    "        tokens_processed = 0; last_log_time = now\n",
    "\n",
    "    # Per-domain eval each skill\n",
    "    py_loss = trainer.evaluate(python_eval, metric_key_prefix='eval_python')['eval_python_loss']\n",
    "    mbpp_loss = trainer.evaluate(mbpp_eval, metric_key_prefix='eval_mbpp')['eval_mbpp_loss']\n",
    "    wandb.log({'eval_python_loss': py_loss, 'eval_mbpp_loss': mbpp_loss})\n",
    "\n",
    "# 20. Final Evaluation on MBPP Test\n",
    "mbpp_test = preprocess_eval(mbpp_dataset['test'], tokenizer)\n",
    "final_mbpp = trainer.evaluate(mbpp_test, metric_key_prefix='final_mbpp')\n",
    "print('Final MBPP test metrics:', final_mbpp)\n",
    "wandb.log({'final_mbpp_loss': final_mbpp['final_mbpp_loss']})\n",
    "\n",
    "# 21. Save Model\n",
    "trainer.save_model(cfg['output_dir'] + '/best_model')\n",
    "tokenizer.save_pretrained(cfg['output_dir'] + '/best_model')\n",
    "print('Saved model and tokenizer to', cfg['output_dir'] + '/best_model')\n",
    "\n",
    "# 22. Finish\n",
    "wandb.finish(); print('Done.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
